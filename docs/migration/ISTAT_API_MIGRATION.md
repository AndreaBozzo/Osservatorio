# ISTAT API Migration Guide

## Migration from IstatAPITester to ProductionIstatClient

This guide covers the migration from the exploration-focused `IstatAPITester` to the production-ready `ProductionIstatClient` introduced in Issue #66.

## Overview

The migration transforms manual, file-based exploration workflows into automated, repository-integrated production processes.

### Before: IstatAPITester (Exploration)
- **Purpose**: Manual testing and exploration
- **Output**: Files (HTML, JSON, PNG, TXT)
- **Usage**: Interactive investigation of ISTAT data
- **Integration**: None (standalone tool)
- **Error Handling**: Basic console output

### After: ProductionIstatClient (Production)
- **Purpose**: Automated data ingestion and processing
- **Output**: Structured data + Repository integration
- **Usage**: Programmatic API access with fault tolerance
- **Integration**: SQLite metadata + DuckDB analytics + FastAPI
- **Error Handling**: Circuit breaker, retry logic, structured logging

## Migration Steps

### Step 1: Update Imports

**Before:**
```python
from src.api.istat_api import IstatAPITester

tester = IstatAPITester()
```

**After:**
```python
from src.api.production_istat_client import ProductionIstatClient
from src.database.sqlite.repository import get_unified_repository

repository = get_unified_repository()
client = ProductionIstatClient(repository=repository)
```

### Step 2: Replace Method Calls

#### API Connectivity Testing

**Before:**
```python
connectivity_results = tester.test_api_connectivity()
for result in connectivity_results:
    if result.get("success"):
        print(f"‚úÖ {result['endpoint']}: OK")
```

**After:**
```python
health = client.health_check()
if health["status"] == "healthy":
    print(f"‚úÖ ISTAT API: OK ({health['response_time']:.2f}s)")
else:
    print(f"‚ùå ISTAT API: {health['error']}")
```

#### Dataset Discovery

**Before:**
```python
datasets = tester.discover_available_datasets(limit=20)
for dataset in datasets:
    print(f"Dataset: {dataset['name']} (ID: {dataset['id']})")
```

**After:**
```python
dataflows = client.fetch_dataflows(limit=20)
for dataflow in dataflows["dataflows"]:
    print(f"Dataset: {dataflow['name']} (ID: {dataflow['id']})")
```

#### Specific Dataset Testing

**Before:**
```python
result = tester.test_specific_dataset("DCIS_POPRES1", "Popolazione residente")
if result.get("data_test", {}).get("success"):
    print("‚úÖ Dataset data retrieved successfully")
```

**After:**
```python
result = client.fetch_dataset("DCIS_POPRES1", include_data=True)
if result.get("data", {}).get("status") == "success":
    print(f"‚úÖ Dataset data: {result['data']['observations_count']} observations")
```

#### Data Quality Validation

**Before:**
```python
quality_report = tester.validate_data_quality("DCIS_POPRES1", sample_size=1000)
if quality_report:
    print(f"Quality Score: {quality_report['quality_score']:.1f}/100")
```

**After:**
```python
quality = client.fetch_with_quality_validation("DCIS_POPRES1")
print(f"Quality Score: {quality.quality_score:.1f}/100")
print(f"Completeness: {quality.completeness}%")
if quality.validation_errors:
    print(f"Issues: {quality.validation_errors}")
```

#### Batch Processing

**Before:**
```python
# Manual loop
for dataset in popular_datasets:
    result = tester.test_specific_dataset(dataset["id"], dataset["name"])
    time.sleep(2)  # Rate limiting
```

**After:**
```python
import asyncio

dataset_ids = [d["id"] for d in popular_datasets]
batch_result = await client.fetch_dataset_batch(dataset_ids)

print(f"Successful: {len(batch_result.successful)}")
print(f"Failed: {len(batch_result.failed)}")
```

### Step 3: Replace File-based Operations

#### Before: Manual File Generation
```python
# tester.create_data_preview_visualization() creates PNG files
# tester.generate_final_report() creates HTML/JSON files
final_report = tester.run_comprehensive_test()
```

#### After: Repository Integration
```python
# Data goes directly to repository
dataset_result = client.fetch_dataset("DCIS_POPRES1")
sync_result = client.sync_to_repository(dataset_result)

print(f"Synced {sync_result.records_synced} records to repository")
```

### Step 4: Error Handling Migration

#### Before: Basic Exception Handling
```python
try:
    result = tester.test_specific_dataset(dataset_id)
except Exception as e:
    print(f"‚ùå Error: {e}")
```

#### After: Structured Error Handling
```python
try:
    result = client.fetch_dataset(dataset_id)
except Exception as e:
    if "Circuit breaker is open" in str(e):
        # API is temporarily unavailable
        print("‚è≥ ISTAT API temporarily unavailable, retrying later...")
        time.sleep(60)
    elif "Rate limit exceeded" in str(e):
        # Too many requests
        print("üêå Rate limit hit, slowing down...")
        time.sleep(300)
    else:
        # Other errors
        print(f"‚ùå API Error: {e}")
```

## Complete Migration Example

### Before: Exploration Script
```python
from src.api.istat_api import IstatAPITester

def explore_istat_data():
    """Old exploration workflow"""
    tester = IstatAPITester()
    
    # Test connectivity
    print("Testing API connectivity...")
    connectivity = tester.test_api_connectivity()
    
    # Discover datasets
    print("Discovering datasets...")
    datasets = tester.discover_available_datasets(limit=10)
    
    # Test specific datasets
    for dataset in datasets[:3]:
        print(f"Testing {dataset['name']}...")
        result = tester.test_specific_dataset(dataset['id'], dataset['name'])
        
        if result.get("data_test", {}).get("success"):
            # Create visualizations
            tester.create_data_preview_visualization(dataset['id'])
            
            # Validate quality
            quality = tester.validate_data_quality(dataset['id'])
            print(f"Quality: {quality['quality_score'] if quality else 'N/A'}")
    
    # Generate final report
    final_report = tester.run_comprehensive_test()
    print("‚úÖ Exploration complete - check generated files")

if __name__ == "__main__":
    explore_istat_data()
```

### After: Production Pipeline
```python
import asyncio
from src.api.production_istat_client import ProductionIstatClient
from src.database.sqlite.repository import get_unified_repository

async def ingest_istat_data():
    """New production workflow"""
    # Initialize production client
    repository = get_unified_repository()
    client = ProductionIstatClient(repository=repository)
    
    # Health check
    health = client.health_check()
    if health["status"] != "healthy":
        raise Exception(f"ISTAT API unavailable: {health['error']}")
    
    # Discover datasets
    print("Discovering datasets...")
    dataflows = client.fetch_dataflows(limit=10)
    print(f"Found {len(dataflows['dataflows'])} datasets")
    
    # Quality filtering and batch processing
    high_quality_datasets = []
    
    for dataflow in dataflows["dataflows"][:5]:  # Process top 5
        dataset_id = dataflow["id"]
        
        # Quality validation
        quality = client.fetch_with_quality_validation(dataset_id)
        
        if quality.quality_score > 80:
            high_quality_datasets.append(dataset_id)
            print(f"‚úÖ {dataset_id}: Quality {quality.quality_score:.1f}/100")
        else:
            print(f"‚ö†Ô∏è  {dataset_id}: Low quality ({quality.quality_score:.1f}/100)")
    
    # Batch ingest high-quality datasets
    if high_quality_datasets:
        batch_result = await client.fetch_dataset_batch(high_quality_datasets)
        
        # Sync successful datasets to repository
        for dataset_id in batch_result.successful:
            dataset_result = client.fetch_dataset(dataset_id)
            sync_result = client.sync_to_repository(dataset_result)
            
            print(f"üìä {dataset_id}: {sync_result.records_synced} records synced")
        
        # Report failures
        for dataset_id, error in batch_result.failed:
            print(f"‚ùå {dataset_id}: {error}")
    
    print("‚úÖ Production ingestion complete")

if __name__ == "__main__":
    asyncio.run(ingest_istat_data())
```

## FastAPI Integration Migration

### Before: Standalone Tool
```python
# Manual execution
python src/api/istat_api.py
```

### After: API Endpoints
```python
# FastAPI endpoints available at:
# GET /api/istat/status
# GET /api/istat/dataflows
# GET /api/istat/dataset/{dataset_id}
# POST /api/istat/sync/{dataset_id}

# Usage with curl:
curl -H "Authorization: Bearer $JWT_TOKEN" \
     "http://localhost:8000/api/istat/dataflows?limit=5"
```

## Configuration Changes

### Environment Variables

**New variables for production client:**
```bash
# Rate limiting
ISTAT_API_RATE_LIMIT=100
ISTAT_API_RATE_WINDOW=3600

# Circuit breaker
ISTAT_CIRCUIT_BREAKER_THRESHOLD=5
ISTAT_CIRCUIT_BREAKER_TIMEOUT=60

# Connection pooling
ISTAT_CONNECTION_POOL_SIZE=10
ISTAT_MAX_POOL_SIZE=20
```

### Repository Integration

**Required for production client:**
```python
# Ensure repository is properly initialized
from src.database.sqlite.repository import get_unified_repository

repository = get_unified_repository()
# Repository provides SQLite metadata + DuckDB analytics integration
```

## Performance Considerations

### Before: Sequential Processing
- One dataset at a time
- Manual rate limiting with `time.sleep()`
- No connection reuse
- File I/O overhead

### After: Optimized Processing
- Concurrent batch processing (5 datasets max)
- Intelligent rate limiting with circuit breaker
- HTTP connection pooling
- Direct repository integration

### Performance Gains

| Operation | Before | After | Improvement |
|-----------|---------|--------|-------------|
| 10 dataset fetch | ~60s sequential | ~15s concurrent | **4x faster** |
| API connectivity | 3 separate requests | 1 pooled request | **3x faster** |
| Data processing | File ‚Üí parse ‚Üí process | Direct processing | **2x faster** |
| Error recovery | Manual restart | Automatic retry | **Fault tolerant** |

## Testing Migration

### Before: Manual Testing
```python
# Run manually and check generated files
tester = IstatAPITester()
report = tester.run_comprehensive_test()
# Check: istat_api_test_report_*.html
```

### After: Automated Testing
```python
# Unit tests
pytest tests/integration/test_production_istat_client.py

# Integration tests with mock server
from tests.integration.mock_istat_server import MockIstatServerContext

with MockIstatServerContext() as server:
    client.base_url = f"{server.get_base_url()}/SDMXWS/rest/"
    result = client.fetch_dataflows()
```

## Monitoring and Observability

### Before: File-based Reports
- HTML reports generated locally
- No real-time monitoring
- Manual review required

### After: Structured Metrics
```python
# Real-time status monitoring
status = client.get_status()
print(f"Circuit Breaker: {status['circuit_breaker_state']}")
print(f"Success Rate: {status['metrics']['successful_requests'] / status['metrics']['total_requests'] * 100:.1f}%")

# Health check endpoint
health = client.health_check()
print(f"API Health: {health['status']}")
```

## Troubleshooting Migration Issues

### Common Issues

#### 1. Import Errors
```python
# Error: ModuleNotFoundError: No module named 'src.api.istat_api'
# Solution: Update imports
from src.api.production_istat_client import ProductionIstatClient
```

#### 2. Repository Not Initialized
```python
# Error: TypeError: ProductionIstatClient() missing repository
# Solution: Initialize repository
from src.database.sqlite.repository import get_unified_repository
repository = get_unified_repository()
client = ProductionIstatClient(repository=repository)
```

#### 3. Async/Await Issues
```python
# Error: RuntimeError: cannot be called from a running event loop
# Solution: Use asyncio.run() or await in async context
import asyncio
result = asyncio.run(client.fetch_dataset_batch(dataset_ids))
```

#### 4. File Dependencies
```python
# Error: Code expects files that no longer exist
# Solution: Use repository data instead
# Before: Parse HTML/JSON files
# After: Query repository directly
datasets = repository.list_datasets()
```

### Validation Checklist

- [ ] All imports updated to `ProductionIstatClient`
- [ ] Repository integration configured
- [ ] File-based operations replaced with repository calls
- [ ] Error handling updated for circuit breaker/rate limiting
- [ ] Async operations properly handled
- [ ] Environment variables configured
- [ ] Tests updated for new client
- [ ] Monitoring/logging configured

## Rollback Plan

If issues arise during migration:

### 1. Keep Old Tool Available
The `IstatAPITester` remains available with deprecation warnings for manual exploration:

```python
# Still works for manual testing
from src.api.istat_api import IstatAPITester
tester = IstatAPITester()  # Shows deprecation warning
```

### 2. Gradual Migration
Migrate functionality incrementally:

```python
# Phase 1: Use new client for data fetching only
client = ProductionIstatClient()
dataflows = client.fetch_dataflows()

# Phase 2: Add quality validation
quality = client.fetch_with_quality_validation(dataset_id)

# Phase 3: Full repository integration
sync_result = client.sync_to_repository(dataset_data)
```

### 3. Feature Flags
Use configuration to control new features:

```python
USE_PRODUCTION_CLIENT = os.getenv("USE_PRODUCTION_CLIENT", "true").lower() == "true"

if USE_PRODUCTION_CLIENT:
    client = ProductionIstatClient(repository=repository)
else:
    client = IstatAPITester()  # Fallback
```

## Support

For migration assistance:
- Check logs: `logs/production_istat_client.log`
- Monitor metrics: `GET /api/istat/status`
- Test connectivity: `client.health_check()`
- Review documentation: `docs/api/PRODUCTION_ISTAT_CLIENT.md`