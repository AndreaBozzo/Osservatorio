# üöÄ Unified Data Pipeline - System Usage Guide

**Issue #63 Implementation - Production Ready**

## üìã Quick Start for Collaborators

### 1. **Testing the System** (5 minutes)

```bash
# Basic system validation
python scripts/validate_production_output.py

# Demo with mock data (safe testing)
python scripts/test_pipeline_demo.py

# Production test with real ISTAT data
python scripts/test_real_data_processing.py
```

### 2. **Processing Single Dataset**

```python
from src.pipeline import PipelineService

# Initialize service
service = PipelineService()
await service.start_background_processing()

# Process single dataset
result = await service.process_dataset(
    dataset_id="DCCN_PILN",
    target_formats=["powerbi", "tableau"],
    fetch_from_istat=True  # or False for local XML
)

print(f"Status: {result.status}")
print(f"Records: {result.records_processed}")
print(f"Quality: {result.quality_score.overall_score}%")
```

### 3. **Batch Processing**

```python
# Process multiple datasets
batch_id = await service.process_multiple_datasets(
    dataset_ids=["DCCN_PILN", "DCIS_POPRES1", "DCCV_TAXOCCU"],
    target_formats=["powerbi"],
    fetch_from_istat=True
)

# Check batch status
batch_status = await service.get_batch_status(batch_id)
print(f"Success rate: {batch_status.success_rate}%")
```

### 4. **Using with Existing XML Files**

```python
# Process local XML file
xml_file = Path("data/raw/istat/istat_data/101_12.xml")
with open(xml_file, 'r', encoding='utf-8') as f:
    xml_content = f.read()

result = await service.pipeline.ingest_dataset(
    dataset_id="101_12_prices",
    sdmx_data=xml_content,
    target_formats=["powerbi"]
)
```

## üéØ **Integration Points**

### With Existing Components

1. **PowerBI Converter Integration**
   - Automatic Excel/CSV/Parquet generation
   - Located: `data/processed/powerbi/`
   - Compatible with existing PowerBI workflows

2. **Tableau Converter Integration**
   - Hyper file generation
   - Located: `data/processed/tableau/`
   - Ready for Tableau consumption

3. **SQLite Metadata** (Issue #59)
   - 25 datasets already configured
   - Automatic metadata updates
   - Database: `data/databases/osservatorio_metadata.db`

4. **Quality Framework** (Issue #3 Ready)
   - Quality hooks implemented
   - Placeholder validation active
   - Ready for Gasta88's implementation

## üìä **Outputs Generated**

### PowerBI Outputs
```
data/processed/powerbi/
‚îú‚îÄ‚îÄ economia_DCCN_PILN_20250717.xlsx    # Excel for PowerBI
‚îú‚îÄ‚îÄ economia_DCCN_PILN_20250717.csv     # CSV backup
‚îú‚îÄ‚îÄ economia_DCCN_PILN_20250717.parquet # Parquet for analytics
‚îî‚îÄ‚îÄ conversion_summary_*.json           # Processing metadata
```

### Performance Data
```
data/performance_results/
‚îî‚îÄ‚îÄ pipeline_demo_report_*.json         # Performance metrics
```

## üîç **Monitoring & Validation**

### System Health Check
```python
status = await service.get_pipeline_status()
print(f"Pipeline: {status['status']}")
print(f"Active jobs: {status['queue']['active_batches']}")
print(f"Performance: {status['performance']['average_throughput']} rec/s")
```

### Quality Validation
```python
quality_result = await service.validate_dataset_quality(
    dataset_id="DCCN_PILN",
    fetch_from_istat=True
)
print(f"Quality: {quality_result['quality_score']['overall']}%")
```

## üö® **Troubleshooting**

### Common Issues

1. **No XML files found**
   ```bash
   # Check ISTAT data directory
   ls data/raw/istat/istat_data/
   # Should show *.xml files
   ```

2. **Pipeline not responding**
   ```python
   # Restart background processing
   await service.start_background_processing()
   ```

3. **Database issues**
   ```bash
   # Check database files
   ls data/databases/
   # Should show osservatorio_metadata.db and osservatorio.duckdb
   ```

4. **Output files missing**
   ```bash
   # Check output directories
   ls data/processed/powerbi/
   ls data/processed/tableau/
   ```

## üîß **Development Workflow**

### For New Features
1. Test with mock data first: `python scripts/test_pipeline_demo.py`
2. Validate with real data: `python scripts/test_real_data_processing.py`
3. Check outputs: `python scripts/validate_production_output.py`
4. Performance check: Review `data/performance_results/`

### For Bug Fixes
1. Reproduce issue with specific dataset
2. Use pipeline service directly for debugging
3. Check logs for detailed error information
4. Validate fix doesn't break existing functionality

## üìà **Performance Expectations**

- **Single dataset**: <1s processing time
- **Batch processing**: ~2-5 datasets/second
- **Memory usage**: ~150MB average
- **Quality scores**: 60-90% typical range
- **API response**: <100ms for status checks

## üîó **Integration with Issue #3**

Quality validation framework is ready:
```python
# Current placeholder implementation
quality_score = await pipeline._validate_data_quality(data, dataset_id)

# Ready for Gasta88's enhancement:
# - Comprehensive quality metrics
# - Configurable thresholds
# - Advanced validation rules
```

## üìû **Support**

1. **System validation**: Use provided test scripts
2. **Integration issues**: Check this guide
3. **Performance problems**: Review performance reports
4. **New requirements**: Extend PipelineService class

---

**‚úÖ The system is PRODUCTION READY for real ISTAT data processing**
**‚úÖ All major components tested and validated**
**‚úÖ Ready for collaborative development and testing**
