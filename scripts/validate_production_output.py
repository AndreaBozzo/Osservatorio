#!/usr/bin/env python3
"""
Production Output Validation - Issue #63

Validate the outputs generated by the unified pipeline with real data.
"""

import json
import sys
from pathlib import Path

import pandas as pd

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.utils.logger import get_logger

logger = get_logger(__name__)


def validate_powerbi_outputs():
    """Validate PowerBI outputs generated by the pipeline."""
    print("🔍 VALIDATING PRODUCTION OUTPUTS")
    print("=" * 40)

    powerbi_dir = Path("data/processed/powerbi")
    if not powerbi_dir.exists():
        print("❌ PowerBI output directory not found")
        return False

    # Check Excel files
    excel_files = list(powerbi_dir.glob("*.xlsx"))
    csv_files = list(powerbi_dir.glob("*.csv"))
    json_files = list(powerbi_dir.glob("*.json"))
    parquet_files = list(powerbi_dir.glob("*.parquet"))

    print("📊 OUTPUT FILES FOUND:")
    print(f"   Excel files: {len(excel_files)}")
    print(f"   CSV files: {len(csv_files)}")
    print(f"   JSON files: {len(json_files)}")
    print(f"   Parquet files: {len(parquet_files)}")

    # Validate data files (non-summary)
    data_files = [f for f in excel_files if not f.name.startswith("conversion_summary")]

    validation_results = []

    for excel_file in data_files[:3]:  # Check first 3 files
        print(f"\n📄 Validating: {excel_file.name}")

        try:
            # Check file size
            file_size = excel_file.stat().st_size
            print(f"   File size: {file_size:,} bytes")

            if file_size == 0:
                print("   ❌ EMPTY FILE")
                validation_results.append(False)
                continue

            # Try to read Excel file
            df = pd.read_excel(excel_file)
            print(f"   Rows: {len(df):,}")
            print(f"   Columns: {len(df.columns)}")

            if len(df) == 0:
                print("   ⚠️  NO DATA ROWS")
                validation_results.append(False)
                continue

            # Check column structure
            print(f"   Columns: {list(df.columns)}")

            # Check for SDMX-like data structure
            expected_cols = ["TIME_PERIOD", "OBS_VALUE"]
            has_time_data = any(col in df.columns for col in expected_cols)

            if has_time_data:
                print("   ✅ VALID SDMX STRUCTURE")

                # Sample data check
                print("   Sample data:")
                for i, row in df.head(2).iterrows():
                    print(f"     Row {i+1}: {dict(row)}")

                validation_results.append(True)
            else:
                print("   ⚠️  NON-STANDARD STRUCTURE")
                validation_results.append(False)

        except Exception as e:
            print(f"   ❌ VALIDATION ERROR: {e}")
            validation_results.append(False)

    # Check corresponding CSV files
    print("\n📊 CSV FILE VALIDATION:")
    for csv_file in csv_files[:3]:
        if csv_file.name.startswith("conversion_summary"):
            continue

        try:
            df_csv = pd.read_csv(csv_file)
            print(
                f"   {csv_file.name}: {len(df_csv)} rows, {len(df_csv.columns)} cols ✅"
            )
        except Exception as e:
            print(f"   {csv_file.name}: ERROR - {e} ❌")

    success_rate = (
        sum(validation_results) / len(validation_results) if validation_results else 0
    )
    print("\n📈 VALIDATION SUMMARY:")
    print(f"   Files validated: {len(validation_results)}")
    print(f"   Success rate: {success_rate*100:.1f}%")

    return success_rate >= 0.8


def validate_conversion_summaries():
    """Validate conversion summary files."""
    print("\n📋 CONVERSION SUMMARIES VALIDATION:")

    powerbi_dir = Path("data/processed/powerbi")
    summary_files = list(powerbi_dir.glob("conversion_summary_*.json"))

    if not summary_files:
        print("   ❌ No conversion summary files found")
        return False

    print(f"   Found {len(summary_files)} summary files")

    # Check latest summary
    latest_summary = max(summary_files, key=lambda f: f.stat().st_mtime)
    print(f"   Latest: {latest_summary.name}")

    try:
        with open(latest_summary, encoding="utf-8") as f:
            summary_data = json.load(f)

        print("   ✅ Valid JSON structure")

        # Check key fields
        if "conversion_time" in summary_data:
            print(f"   Conversion time: {summary_data['conversion_time']}")

        if "datasets_processed" in summary_data:
            print(f"   Datasets processed: {summary_data['datasets_processed']}")

        if "total_records" in summary_data:
            print(f"   Total records: {summary_data['total_records']:,}")

        return True

    except Exception as e:
        print(f"   ❌ SUMMARY VALIDATION ERROR: {e}")
        return False


def validate_performance_data():
    """Validate performance monitoring data."""
    print("\n🚀 PERFORMANCE DATA VALIDATION:")

    perf_dir = Path("data/performance_results")
    if not perf_dir.exists():
        print("   ⚠️  No performance data directory")
        return False

    perf_files = list(perf_dir.glob("*.json"))
    if not perf_files:
        print("   ⚠️  No performance data files")
        return False

    print(f"   Found {len(perf_files)} performance files")

    # Check latest performance report
    latest_perf = max(perf_files, key=lambda f: f.stat().st_mtime)
    print(f"   Latest: {latest_perf.name}")

    try:
        with open(latest_perf, encoding="utf-8") as f:
            perf_data = json.load(f)

        print("   ✅ Valid performance JSON")

        if "performance_summary" in perf_data:
            summary = perf_data["performance_summary"]
            if "average_throughput" in summary:
                print(
                    f"   Average throughput: {summary['average_throughput']:.1f} rec/s"
                )
            if "average_duration_seconds" in summary:
                print(
                    f"   Average duration: {summary['average_duration_seconds']:.3f}s"
                )
            if "error_rate_percent" in summary:
                print(f"   Error rate: {summary['error_rate_percent']:.1f}%")

        return True

    except Exception as e:
        print(f"   ❌ PERFORMANCE VALIDATION ERROR: {e}")
        return False


def check_database_integration():
    """Check database integration."""
    print("\n🗄️  DATABASE INTEGRATION CHECK:")

    # Check SQLite database
    db_path = Path("data/databases/osservatorio_metadata.db")
    if db_path.exists():
        db_size = db_path.stat().st_size
        print(f"   SQLite DB: {db_size:,} bytes ✅")
    else:
        print("   SQLite DB: NOT FOUND ❌")
        return False

    # Check DuckDB database
    duckdb_path = Path("data/databases/osservatorio.duckdb")
    if duckdb_path.exists():
        duckdb_size = duckdb_path.stat().st_size
        print(f"   DuckDB: {duckdb_size:,} bytes ✅")
    else:
        print("   DuckDB: NOT FOUND ❌")
        return False

    return True


def main():
    """Run production output validation."""
    print("🏭 PRODUCTION OUTPUT VALIDATION - Issue #63")
    print("=" * 60)

    validation_results = []

    # Test 1: PowerBI outputs
    powerbi_valid = validate_powerbi_outputs()
    validation_results.append(("PowerBI Outputs", powerbi_valid))

    # Test 2: Conversion summaries
    summaries_valid = validate_conversion_summaries()
    validation_results.append(("Conversion Summaries", summaries_valid))

    # Test 3: Performance data
    performance_valid = validate_performance_data()
    validation_results.append(("Performance Data", performance_valid))

    # Test 4: Database integration
    db_valid = check_database_integration()
    validation_results.append(("Database Integration", db_valid))

    # Final assessment
    print("\n" + "=" * 60)
    print("🎯 PRODUCTION VALIDATION SUMMARY")
    print("=" * 60)

    passed = sum(1 for _, valid in validation_results if valid)
    total = len(validation_results)

    for test_name, valid in validation_results:
        status = "✅ PASS" if valid else "❌ FAIL"
        print(f"   {test_name}: {status}")

    success_rate = (passed / total) * 100
    print(f"\n📊 Overall Success Rate: {success_rate:.1f}% ({passed}/{total})")

    if success_rate >= 90:
        print("🏆 EXCELLENT - PRODUCTION READY")
        print("✅ All critical components validated")
        print("✅ Real data processing confirmed")
        print("✅ Output generation working")
        print("✅ Performance monitoring active")
        print("✅ Database integration stable")
    elif success_rate >= 75:
        print("⚡ GOOD - Minor issues to address")
        print("✅ Core functionality working")
        print("⚠️  Some components need attention")
    else:
        print("❌ CRITICAL ISSUES - Not production ready")
        print("🛑 Multiple validation failures")

    print("\n🔗 Issue #63 Production Status:")
    if success_rate >= 75:
        print("✅ Unified Data Ingestion Pipeline - PRODUCTION VALIDATED")
        print("✅ Real ISTAT data processing - CONFIRMED")
        print("✅ Multi-format output generation - WORKING")
        print("✅ Foundation architecture - STABLE")
    else:
        print("⚠️  Production validation needs improvement")

    return success_rate >= 75


if __name__ == "__main__":
    main()
